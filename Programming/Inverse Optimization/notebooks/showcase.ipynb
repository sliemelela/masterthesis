{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code below, we ensure that we are constantly updating the local packages while importing them in the jupyter notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from inverse_optim import gen_data\n",
    "from inverse_optim import research_plot\n",
    "from inverse_optim import sancho\n",
    "import tadasets\n",
    "import powerbox as pbox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circle and the Figure Eight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, choose which figure you would like to generate. You can comment out the figure you are not interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a synthetic data set that we want \"approximate\"\n",
    "N = 300\n",
    "goal_pts = tadasets.dsphere(n=N, d=1, noise=0.1) # circle\n",
    "# goal_pts = tadasets.infty_sign(n=N, noise=0.1) # figure eight\n",
    "\n",
    "# To perform Stochastic Gradient Descent (SGD), we need our set to be of tensor type\n",
    "goal_pts = torch.tensor(goal_pts)\n",
    "\n",
    "# Plot the initial/goal data set\n",
    "P = goal_pts.detach().numpy()\n",
    "plt.scatter(P[:, 0], P[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Filtration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ripser Filtration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha DTM Filtration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha-Ripser Hybrid Filtration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the PD that we want to get to\n",
    "goal_pd = gen_data.create_alpha_pd(goal_pts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to find out what optimal learning rate is. If you would like to make the computation quicker (which will make it less accurate), pass the option sliced=True to use the sliced wasserstein distance as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = np.linspace(0.001, 0.2, 6)\n",
    "research_plot.research_lr(lr_list=lr_list, goal_pd=goal_pd, amount=N, dim=2, epochs=300, decay_speed=30, sliced=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code, plug in the best learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of new dataset\n",
    "final_pts = gen_data.generate_data(goal_pd=goal_pd, amount=N, dim=2, lr=0.08, epochs=600, decay_speed=30, investigate=False, sliced=False, filtr=\"alpha_rips_hybrid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The powerspectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pts = goal_pts.detach().numpy()\n",
    "produced_pts = final_pts.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of grid points are also required when passing the samples\n",
    "p_k_samples, bins_samples = pbox.get_power(original_pts, 2.0, N=N)\n",
    "p_k_samples_new, bins_samples_new = pbox.get_power(produced_pts, 8.0, N=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bins_samples, p_k_samples,label=\"Original Circle Power\")\n",
    "plt.plot(bins_samples_new, p_k_samples_new,label=\"Generated Circle Power\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sancho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "cat = np.load(f'/Users/sliemela/Downloads/Sancho/fiducial_HOD_fid_NFW_sample0_1Gpc_z0.50_RSD3_run0.npz')\n",
    "pos = cat['pos']        # shape: (N_galaxies, 3) --> X,Y,Z position of each galaxy in Mpc/h\n",
    "vel = cat['vel']        # shape: (N_galaxies, 3) --> Vx, Vy, Vz velocity of the galaxy in km/s\n",
    "gtype = cat['gtype']\n",
    "\n",
    "# Split up the dataset\n",
    "split = (2,2,2)\n",
    "bins = sancho.bin(pos, split)\n",
    "\n",
    "# Plotting the bins\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "for x in bins:\n",
    "    if len(x) != 0:\n",
    "        ax.scatter3D(x[:, 0], x[:, 1], x[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the statistics of the wasserstein distances of sancho\n",
    "list_of_wasser_dist = sancho.compare_wasser_alpha(bins)\n",
    "\n",
    "wasser_mean = np.mean(list_of_wasser_dist)\n",
    "wasser_std = np.std(list_of_wasser_dist)\n",
    "\n",
    "print(wasser_mean)\n",
    "print(wasser_std)\n",
    "\n",
    "# NOTE: the code of compare_wasser_alpha has been changed to only consider the first 3000. In the future, we may consider all pairs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Inverse Optimization Virtual Environment",
   "language": "python",
   "name": "inverse_optim_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
