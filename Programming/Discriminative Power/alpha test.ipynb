{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "u446jHdrEK3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this experiment, we use synthetized data. The goal is to retrieve parameters of dynamical system orbits, following an experiment proposed in https://jmlr.org/papers/volume18/16-337/16-337.pdf.\n",
        "\n",
        "For this experiment we create datasets $\\{(x_n, y_n) \\mid n \\in \\mathbb{N} \\}$ using the following recursive formula:\n",
        "$$\n",
        "\\begin{cases}\n",
        "x_{n+1} &= x_n + r \\cdot (1 - y_n) \\mod 1\\\\\n",
        "y_{n+1} &= y_n + r \\cdot (1 - x_{n+1}) \\mod 1\n",
        "\\end{cases}\n",
        "$$\n",
        "Note that there are two parameters needed to create this dataset: the initial point $(x_0, y_0)$ and the so-called 'dynamical parameter' $r$.\n",
        "Choosing different initial points $(x_0, y_0)$ for fixed $r$ will result in datasets that look fairly similar. However, changing the dynamical parameter $r$ will usually result in wildly different behaviours.\n",
        "\n",
        "The goal of this notebook is to create a classification pipeline which leverages multiparameter persistence is some way to demonstrate the discriminative power that multiparameter persistence has to offer.\n",
        "For this we create the dynamical parameters of interest are $r = 2.5, 3.5, 4.0, 4.1$ and $4.3$. For each $r$ we choose $50$ randomly chosen $(x_0, y_0) \\in [0,1] \\times [0,1]$ after which we created the truncated orbits $\\{(x_n, y_n) \\mid n \\in [1000]\\}$."
      ],
      "metadata": {
        "id": "8LZwyz8TEQzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start up"
      ],
      "metadata": {
        "id": "fZ8m9HTQEXXF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWc0aHECDtTv",
        "outputId": "2f6ed6c7-4f93-4b79-aeea-39acfec063d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gudhi\n",
            "  Downloading gudhi-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/3.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/3.0 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from gudhi) (1.23.5)\n",
            "Installing collected packages: gudhi\n",
            "Successfully installed gudhi-3.8.0\n",
            "Collecting eagerpy\n",
            "  Downloading eagerpy-0.30.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from eagerpy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from eagerpy) (4.5.0)\n",
            "Installing collected packages: eagerpy\n",
            "Successfully installed eagerpy-0.30.0\n",
            "Collecting tadasets\n",
            "  Downloading tadasets-0.0.4-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tadasets) (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from tadasets) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tadasets) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tadasets) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tadasets) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tadasets) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tadasets) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tadasets) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tadasets) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tadasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->tadasets) (1.16.0)\n",
            "Installing collected packages: tadasets\n",
            "Successfully installed tadasets-0.0.4\n",
            "Collecting POT\n",
            "  Downloading POT-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (789 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m790.0/790.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from POT) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from POT) (1.11.3)\n",
            "Installing collected packages: POT\n",
            "Successfully installed POT-0.9.1\n"
          ]
        }
      ],
      "source": [
        "%pip install gudhi\n",
        "%pip install eagerpy\n",
        "%pip install tadasets\n",
        "%pip install POT\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import gudhi as gd\n",
        "import matplotlib.pyplot as plt\n",
        "import tadasets\n",
        "from gudhi.representations.vector_methods import PersistenceImage\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.spatial import ConvexHull\n",
        "from sklearn.neighbors import KDTree\n",
        "from tqdm import tqdm\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creation of the synthesized dataset\n",
        "\n",
        "We are going to create and organise the different datasets in the following manner.\n",
        "- Each row is a different datasets\n",
        "- The first column will have the label. The rest of the columns will contain the coordinates. So the columns are ordered in the following manenr:\n",
        "$$\n",
        "(\\text{label}, x_0, x_1, \\ldots, x_{N - 1}, y_0, \\ldots, y_{N-1}),\n",
        "$$\n",
        "where $N$ denotes the amount of datapoints.\n",
        "\n"
      ],
      "metadata": {
        "id": "vkrk35buEcYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed for reproducibility\n",
        "np.random.seed(1)\n",
        "\n",
        "# Functions needed for creation of dataset\n",
        "def fx(x, y, r):\n",
        "  return (x + r * y * (1 - y)) % 1\n",
        "\n",
        "def fy(x, y, r):\n",
        "  return (y + r * x * (1 - x)) % 1\n",
        "\n",
        "# Initial conditions\n",
        "r_list = [2., 3.5, 4., 4.1, 4.3]\n",
        "label_amount = len(r_list)\n",
        "per_label_amount = 50\n",
        "data_total_amount = 1000\n",
        "no_noise_amount = 1000\n",
        "\n",
        "noise_amount = data_total_amount - no_noise_amount\n",
        "\n",
        "coord_list = []\n",
        "for label, r in enumerate(r_list):\n",
        "\n",
        "  # Random starting points\n",
        "  x0 = np.random.rand(per_label_amount)\n",
        "  y0 = np.random.rand(per_label_amount)\n",
        "\n",
        "  xlist = [x0]\n",
        "  ylist = [y0]\n",
        "\n",
        "  for i in range(no_noise_amount - 1):\n",
        "    # Creation of new datapoint\n",
        "    xn = fx(xlist[i], ylist[i], r)\n",
        "    yn = fy(xn, ylist[i], r)\n",
        "\n",
        "    # Keeping track of data\n",
        "    xlist.append(xn)\n",
        "    ylist.append(yn)\n",
        "\n",
        "  for i in range(noise_amount):\n",
        "\n",
        "    # Creation of random datapoint\n",
        "    xn = np.random.rand(per_label_amount)\n",
        "    yn = np.random.rand(per_label_amount)\n",
        "\n",
        "    # Keeping track of data\n",
        "    xlist.append(xn)\n",
        "    ylist.append(yn)\n",
        "\n",
        "  # A collection of all x and y coordinates\n",
        "  xlist = np.stack(xlist).T\n",
        "  ylist = np.stack(ylist).T\n",
        "\n",
        "  # Concatenate the x, y coordinates and the label\n",
        "  coords_pre = np.concatenate((xlist, ylist), 1)\n",
        "  labels = np.full((per_label_amount,1), float(label))\n",
        "  coords = np.concatenate((labels, coords_pre), 1)\n",
        "\n",
        "  coord_list.append(coords)\n",
        "\n",
        "coords = np.concatenate(coord_list)\n",
        "coords = coords.astype(np.float32)"
      ],
      "metadata": {
        "id": "VpN8I3jSEeiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation of the $\\gamma$-filtration function and the creation of persistence diagrams"
      ],
      "metadata": {
        "id": "_PxqAojLMuiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the functions"
      ],
      "metadata": {
        "id": "EVm_fJ7-OBJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we create the function that traces a path in the bipersistence module dubbed $\\gamma$ (gamma).\n",
        "We also need its inverse to calculate the filtration value."
      ],
      "metadata": {
        "id": "GAbJ2R1CM-zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gamma(t, radius_subd, parameters):\n",
        "\n",
        "    # In case t is torch tensor\n",
        "    if isinstance(t, torch.Tensor):\n",
        "\n",
        "        # Find out where t falls in the subdivision\n",
        "        indices = (torch.searchsorted(radius_subd, t, right=True) - 1)\n",
        "\n",
        "        # Extract alphas and differences of alphas\n",
        "        alphas = parameters[1:]\n",
        "        alphasdif = alphas[:-1] - alphas[1:]\n",
        "\n",
        "        # Prepare the resulting tensor of outputs\n",
        "        result = torch.zeros_like(t).to(radius_subd.device)\n",
        "\n",
        "        # Divide times into two cases\n",
        "        mask1 = (indices <= 0)\n",
        "        mask2 = ~mask1\n",
        "\n",
        "        # Calculate the sums of differences times alphas times the radii\n",
        "        sums = torch.cumsum(alphasdif * radius_subd[1:], dim=0)\n",
        "\n",
        "        # Compute result\n",
        "        result[mask1] = parameters[1] * t[mask1] + parameters[0]\n",
        "        result[mask2] = parameters[indices[mask2] + 1] * t[mask2] + sums[indices[mask2] - 1] + parameters[0]\n",
        "\n",
        "        return result\n",
        "\n",
        "    # Find out where t falls in the subdivision\n",
        "    indices = np.searchsorted(radius_subd, t, side='right') - 1\n",
        "\n",
        "    # Extract alphas and differences of alphas\n",
        "    alphas = parameters[1:]\n",
        "    alphasdif = alphas[:-1] - alphas[1:]\n",
        "\n",
        "    # Perpare the resulting vector of outputs\n",
        "    result = np.zeros_like(t)\n",
        "\n",
        "    # Divide times into two cases\n",
        "    mask1 = (indices <= 0)\n",
        "    mask2 = ~mask1\n",
        "\n",
        "    # Convert t into an array it is a scalar\n",
        "    if np.isscalar(t):\n",
        "        t = np.array(t)\n",
        "\n",
        "    # Calculate the sums of differences times alphas times the radii\n",
        "    sums = np.cumsum(alphasdif * radius_subd[1:])\n",
        "\n",
        "    # Compute result\n",
        "    result[mask1] = parameters[1] * t[mask1] + parameters[0]\n",
        "    result[mask2] = parameters[indices[mask2] + 1]*t[mask2] +  sums[indices[mask2] - 1] + parameters[0]\n",
        "    return result\n",
        "\n",
        "def gamma_inverse(y, radius_subd, parameters):\n",
        "\n",
        "    density_subd = gamma(radius_subd, radius_subd, parameters)\n",
        "\n",
        "    # Treat the case when y is a tensor\n",
        "    if isinstance(y, torch.Tensor):\n",
        "\n",
        "        # Find out where t falls in the subdivision\n",
        "        indices = torch.searchsorted(density_subd, y, right=True) - 1\n",
        "\n",
        "        # Extract alphas and differences of alphas\n",
        "        alphas = parameters[1:]\n",
        "        alphasdif = alphas[:-1] - alphas[1:]\n",
        "\n",
        "        # Prepare the resulting tensor of outputs\n",
        "        result = torch.zeros_like(y).to(radius_subd.device)\n",
        "\n",
        "        # Divide times into two cases\n",
        "        mask1 = (indices <= 0)\n",
        "        mask2 = ~mask1\n",
        "\n",
        "        # Calculate the sums of differences times alphas times the radii\n",
        "        sums = torch.cumsum(alphasdif * radius_subd[1:], dim=0)\n",
        "\n",
        "        # Compute result\n",
        "        result[mask1] = 1 / parameters[1] * y[mask1] - parameters[0] / parameters[1]\n",
        "        result[mask2] = 1 / parameters[indices[mask2] + 1] * y[mask2] - (sums[indices[mask2] - 1] + parameters[0]) / parameters[indices[mask2] + 1]\n",
        "\n",
        "        return result\n",
        "    # Find out where t falls in the subdivision\n",
        "    indices = np.searchsorted(density_subd, y, side='right') - 1\n",
        "\n",
        "    # Extract alphas and differences of alphas\n",
        "    alphas = parameters[1:]\n",
        "    alphasdif = alphas[:-1] - alphas[1:]\n",
        "\n",
        "    # Perpare the resulting vector of outputs\n",
        "    result = np.zeros_like(y)\n",
        "\n",
        "    # Divide times into two cases\n",
        "    mask1 = (indices <= 0)\n",
        "    mask2 = ~mask1\n",
        "\n",
        "    # Convert t into an array it is a scalar\n",
        "    if np.isscalar(y):\n",
        "        y = np.array(y)\n",
        "\n",
        "    # Calculate the sums of differences times alphas times the radii\n",
        "    sums = np.cumsum(alphasdif * radius_subd[1:])\n",
        "\n",
        "    # Compute result\n",
        "    result[mask1] = 1/parameters[1] * y[mask1]  - parameters[0]/parameters[1]\n",
        "    result[mask2] = 1/parameters[indices[mask2] + 1] * y[mask2] - (sums[indices[mask2] - 1] + parameters[0])/parameters[indices[mask2] + 1]\n",
        "    return result"
      ],
      "metadata": {
        "id": "-Yu_7u6BIDmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here we need to define some helper functions to be able to calculate the filtration values of the persistence diagram."
      ],
      "metadata": {
        "id": "Fb9EP7SgNJJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to create pd\n",
        "def DTM(X,query_pts,m):\n",
        "    '''\n",
        "    Compute the values of the DTM (with exponent p=2) of the empirical measure of a point cloud X.\n",
        "    For Pytorch GPU acceleration, make sure X and query_pts are stored on the GPU\n",
        "\n",
        "    Input:\n",
        "    X: a nxd torch tensor or numpy array representing n points in R^d\n",
        "    query_pts:  a kxd torch tensor or numpy array of query points\n",
        "    m: parameter of the DTM in [0,1)\n",
        "\n",
        "    Output:\n",
        "    DTM_result: a kx1 torch tensor or numpy array contaning the DTM of the query points\n",
        "\n",
        "    Example:\n",
        "    X = torch.tensor([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
        "    Q = torch.tensor([[0,0],[5,5]])\n",
        "    DTM_values = DTM(X, Q, 0.3)\n",
        "    '''\n",
        "\n",
        "    if isinstance(X, torch.Tensor):\n",
        "        if type(X) is not type(query_pts):\n",
        "            raise \"The query_pts and should be of the same type as the reference point set X.\"\n",
        "\n",
        "        # Computation of number of neighbors\n",
        "        N_tot = X.size(dim=0)\n",
        "        k = math.floor(m*N_tot) + 1\n",
        "\n",
        "        # Computation of pairwise distance of all points\n",
        "        distances = torch.cdist(X, query_pts)\n",
        "\n",
        "        # Obtains k indices of the small distances\n",
        "        _, indices = distances.topk(k, dim=1, largest=False)\n",
        "        NN_Dist = torch.gather(distances, 1, indices)\n",
        "\n",
        "        # Calculation of DTM values\n",
        "        DTM_result = torch.sqrt(torch.sum(NN_Dist**2, dim=1) / k)\n",
        "\n",
        "        return DTM_result\n",
        "    else:\n",
        "        if type(X) is not type(query_pts):\n",
        "            \"The query_pts and should be of the same type as the reference point set X.\"\n",
        "\n",
        "        N_tot = X.shape[0]\n",
        "        k = math.floor(m*N_tot)+1   # number of neighbors\n",
        "\n",
        "        kdt = KDTree(X, leaf_size=30, metric='euclidean')\n",
        "        NN_Dist, NN = kdt.query(query_pts, k, return_distance=True)\n",
        "\n",
        "        DTM_result = np.sqrt(np.sum(NN_Dist*NN_Dist,axis=1) / k)\n",
        "\n",
        "        return DTM_result\n",
        "\n",
        "def s(simplex, DTM_values):\n",
        "  return torch.max(DTM_values[simplex])\n",
        "\n",
        "def orig_filt(simplex, pts):\n",
        "    if len(simplex) == 1:\n",
        "        return torch.tensor(0.0)\n",
        "    else:\n",
        "        simplex_coords = pts[simplex]\n",
        "        # try:\n",
        "        #     # Points with the largest distance are in the convex hull\n",
        "        #     hull = ConvexHull(simplex_coords)\n",
        "\n",
        "        #     # Extract the points forming the hull\n",
        "        #     hullpoints = simplex_coords[hull.vertices,:]\n",
        "\n",
        "        #     # Naive way of finding the best pair in O(H^2) time if H is number of points on hull\n",
        "        #     hdist = cdist(hullpoints, hullpoints, metric='euclidean')\n",
        "\n",
        "        #     # Get the farthest apart points\n",
        "        #     bestpair = np.unravel_index(hdist.argmax(), hdist.shape)\n",
        "\n",
        "        #     # Calculate the distance between the furthest pair\n",
        "        #     distance = np.linalg.norm(hullpoints[bestpair[0]] - hullpoints[bestpair[1]])\n",
        "\n",
        "        #     return torch.tensor(distance)\n",
        "        # except:\n",
        "\n",
        "        if isinstance(simplex_coords, torch.Tensor):\n",
        "          # Calculate all pairwise distances\n",
        "          alldist = torch.cdist(simplex_coords, simplex_coords)\n",
        "\n",
        "          # Get the maximum distance and its indices\n",
        "          max_distance = torch.max(alldist)\n",
        "          indices = torch.where(alldist == max_distance)\n",
        "\n",
        "          # Get the farthest apart points\n",
        "          point1 = simplex_coords[indices[0][0]]\n",
        "          point2 = simplex_coords[indices[1][0]]\n",
        "\n",
        "          # Calculate the distance between the furthest pair\n",
        "          distance = torch.dist(point1, point2)\n",
        "\n",
        "          return distance\n",
        "\n",
        "        # Naive way of finding the best pair in O(H^2) time if H is number of points on hull\n",
        "        alldist = cdist(simplex_coords, simplex_coords, metric='euclidean')\n",
        "\n",
        "        # Get the farthest apart points\n",
        "        bestpair = np.unravel_index(alldist.argmax(), alldist.shape)\n",
        "\n",
        "        # Calculate the distance between the furthest pair\n",
        "        distance = np.linalg.norm(simplex_coords[bestpair[0]] - simplex_coords[bestpair[1]])\n",
        "        return torch.tensor(distance)\n",
        "\n",
        "def filtration_function(parameters, time_subd, simplex, DTM_values, pts):\n",
        "    if len(simplex) == 0:\n",
        "        return torch.tensor(float('inf'))\n",
        "\n",
        "    dens = s(simplex, DTM_values)\n",
        "    orig_filt_value = orig_filt(simplex, pts)\n",
        "    gamma_inv = gamma_inverse(dens, time_subd, parameters)\n",
        "    gamma_value = gamma(orig_filt_value, time_subd, parameters)\n",
        "\n",
        "\n",
        "    if orig_filt_value >= gamma_inv:\n",
        "        rhs = gamma_value - parameters[0]\n",
        "\n",
        "        # Convert everything to 1 dimensional tensor\n",
        "        rhs = rhs.unsqueeze(0)\n",
        "        lhs = orig_filt_value.unsqueeze(0)\n",
        "\n",
        "        # Calculate filtration value\n",
        "        vec = torch.cat((lhs, rhs))\n",
        "        filtration = torch.linalg.norm(vec, -float('inf'))\n",
        "        return filtration\n",
        "    else:\n",
        "        lhs = gamma_inv.unsqueeze(0)\n",
        "        rhs = (dens - parameters[0]).unsqueeze(0)\n",
        "\n",
        "        vec = torch.cat((lhs, rhs))\n",
        "        filtration = torch.linalg.norm(vec, -float('inf'))\n",
        "        return filtration"
      ],
      "metadata": {
        "id": "GOkGODE5NU2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we create the function that creates the persistence diagram. The assignment of the filtration values (more precisely, the creation of the simplex tree), happens in the code for `gamma_pd` instead of having a seperate `gamma_st` function. If we would make a seperate `gamma_st` function, we would have to repeat the calculation of the filtration value to make it differentiable for torch, hence we do everything at once."
      ],
      "metadata": {
        "id": "uMrC5EoeNa27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gamma_pd(parameters, time_subd, pts, m=0.5, dimension_max=2):\n",
        "  \"\"\"\n",
        "  Creation of persistence diagram by drawing a path of line-segments in the\n",
        "  bipersistence module M_{(t,y)} = A(f^{-1}[-\\infty, t))_y where f is the DTM\n",
        "  function.\n",
        "\n",
        "  Args:\n",
        "    - parameters  : an array/tensor of shape (n, )\n",
        "    - time_subd   : an array/tensor of shape (n-1,)\n",
        "    - pts         : an array/tensor of shape (N, 2)\n",
        "    - m           : float in interval [0,1)\n",
        "    - dimension_max\n",
        "                  : integer >= 1\n",
        "\n",
        "  Example:\n",
        "    parameters = torch.tensor([0.1, 0.5, 0.5, 3., 4.])\n",
        "    time_subd  = torch.tensor([0., 0.1, 0.2, 0.3])\n",
        "    pts        = torch.rand(200,2, dtype=torch.float32)\n",
        "  \"\"\"\n",
        "\n",
        "  # Build the alpha complex\n",
        "  alpha_complex = gd.AlphaComplex(points=pts)\n",
        "  alpha_st = alpha_complex.create_simplex_tree()\n",
        "\n",
        "    # Calculating persistence\n",
        "  alpha_st.compute_persistence(2)\n",
        "  p = alpha_st.persistence_pairs()\n",
        "\n",
        "  # Keep only pairs that contribute to H1, i.e. (edge, triangle), and separate birth (p1b) and death (p1d)\n",
        "  p1b = torch.tensor([i[0] for i in p if len(i[0]) == 2])\n",
        "  p1d = torch.tensor([i[1] for i in p if len(i[0]) == 2])\n",
        "\n",
        "  # Keep only pairs that contribute to H0, i.e. (vertex, edge), and separate birth (p1b0) and death (p1d0)\n",
        "  # Skipping the infinities by checking second part instead of first part.\n",
        "  p0b = torch.tensor([i[0] for i in p if len(i[1]) == 2])\n",
        "  p0d = torch.tensor([i[1] for i in p if len(i[1]) == 2])\n",
        "\n",
        "  # Compute the distance between the extremities of the birth edge for H1\n",
        "  if len(p1b) == 0:\n",
        "      diag1 = torch.tensor([])\n",
        "  else:\n",
        "      b = torch.norm(pts[p1b[:,1]] - pts[p1b[:,0]], dim=-1, keepdim=True)\n",
        "\n",
        "      if len(p1d) == 0:\n",
        "          d = torch.tensor([float('inf')])\n",
        "      else:\n",
        "          # For the death triangle, compute the maximum of the pairwise distances\n",
        "          d_1 = torch.norm(pts[p1d[:,1]] - pts[p1d[:,0]], dim=-1, keepdim=True)\n",
        "          d_2 = torch.norm(pts[p1d[:,1]] - pts[p1d[:,2]], dim=-1, keepdim=True)\n",
        "          d_3 = torch.norm(pts[p1d[:,2]] - pts[p1d[:,0]], dim=-1, keepdim=True)\n",
        "          d = torch.max(d_1, torch.max(d_2, d_3))\n",
        "\n",
        "      # *Not* the same as the finite part of st.persistence_intervals_in_dimension(1)\n",
        "      diag1 = torch.cat((b,d), 1)\n",
        "\n",
        "  # Compute the distance between the extremities of the birth edge for H0\n",
        "  if len(p0b) == 0:\n",
        "      diag0 = torch.tensor([])\n",
        "  else:\n",
        "      # All birth times are 0 for the zero dimensional features\n",
        "      # b0 = torch.norm(pts[p0b[:,1]] - pts[p0b[:,0]], dim=-1, keepdim=True)\n",
        "      b0 = torch.tensor([[0] for _ in range(len(p0b))])\n",
        "\n",
        "      # Calculate the death times\n",
        "      d0 = torch.norm(pts[p0d[:,1]] - pts[p0d[:,0]], dim=-1, keepdim=True)\n",
        "\n",
        "      diag0 = torch.cat((b0,d0), 1)\n",
        "\n",
        "  diag0 = diag0.numpy()\n",
        "  diag1 = diag1.numpy()\n",
        "\n",
        "\n",
        "  return [diag0, diag1]"
      ],
      "metadata": {
        "id": "U7_hNQWENWM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creation of the persistence diagrams"
      ],
      "metadata": {
        "id": "dfooA5GTOFRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the above functions, we would like to create a function that inputs the point sets (and some parameters) that immediately gives us all the persistence diagrams."
      ],
      "metadata": {
        "id": "MbPqJM1JOafx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pd_collec(coords, parameters, time_subd, m=0.1):\n",
        "\n",
        "  # Obtain information about the coords so that we can extract point set per r\n",
        "  data_total_amount = int((coords.shape[1] - 1)/2)\n",
        "\n",
        "  # Create point set for each coordinate\n",
        "  diag0_list = []\n",
        "  diag1_list = []\n",
        "  for coord in coords:\n",
        "\n",
        "    # Extract x and y coordinates and combine into point set\n",
        "    x = coord[1 : data_total_amount+1]\n",
        "    y = coord[data_total_amount + 1: ]\n",
        "    pts = torch.stack((x, y), dim=-1)\n",
        "\n",
        "    # Creation of diagram\n",
        "    diag0, diag1 = gamma_pd(parameters, time_subd, pts, m)\n",
        "    diag0_list.append(diag0)\n",
        "    diag1_list.append(diag1)\n",
        "\n",
        "  return diag0_list, diag1_list"
      ],
      "metadata": {
        "id": "mGFQN-WHOP-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the loss"
      ],
      "metadata": {
        "id": "UpON9sNSqbbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now want to create a loss function that makes the distance larger smaller diagrams of the same label and normalize that somehow.\n",
        "The values of the persistent diagram depend on our choice of our parameters, represented by the vector $\\alpha$.\n",
        "\n",
        "So our loss is given by\n",
        "$$\n",
        "L(\\alpha) = \\sum_{l = 1}^{N} \\frac{\\sum_{i,j: y_i = y_j = l} SW_p(D^0_i(\\alpha), D^0_j(\\alpha))}{\\sum_{i,j: y_i = l} SW_p(D^0_i(\\alpha), D^0_j(\\alpha))} + \\sum_{l = 1}^{N} \\frac{\\sum_{i,j: y_i = y_j = l} SW_p(D^1_i(\\alpha), D^1_j(\\alpha))}{\\sum_{i,j: y_i = l} SW_p(D^1_i(\\alpha), D^1_j(\\alpha))}.\n",
        "$$\n",
        "Here $l$ represents the label (which in our case can be different values of $r$), the indices $i,j$ in the sums in the fraction represent the $i$-th and $j$-th dataset respectively, $D^k_i(\\alpha)$ is the persistence diagram of the $i$-th dataset with the $\\gamma_\\alpha$ filtration of dimension $k$ and $y_i$ is the label of the $i$-th dataset."
      ],
      "metadata": {
        "id": "jcTd5p61qdzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def general_sliced_wasserstein_distance(dgms, thetas):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        dgms: list of persistent diagrams\n",
        "        ccards: cumulative sum of diagram cardinalities (ccards = np.cumsum([0]+[dgm.shape[0] for dgm in dgms]))\n",
        "        thetas: angles parametrizing the lines\n",
        "\n",
        "    Returns:\n",
        "        - Sliced wasserstein distance\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert ccards and thetas to tensor in case it is not a tensor\n",
        "    ccards = torch.tensor(np.cumsum([0] + [dgm.shape[0] for dgm in dgms])).to(thetas.device)\n",
        "\n",
        "    dgm_cat = torch.cat(dgms,dim=0).to(torch.float32).to(thetas.device)\n",
        "    projected_dgms = torch.matmul(dgm_cat, .5*torch.ones(size=(2,2), dtype=torch.float32).to(thetas.device))\n",
        "    dgms_temp = [torch.reshape(\n",
        "        torch.cat([dgm, projected_dgms[:ccards[idg]], projected_dgms[ccards[idg+1]:]], dim=0), \\\n",
        "            [-1,2,1,1]) for idg, dgm in enumerate(dgms)]\n",
        "    dgms_big = torch.cat(dgms_temp, dim=2)\n",
        "    cosines, sines = torch.cos(thetas), torch.sin(thetas)\n",
        "    vecs = torch.cat([torch.reshape(cosines,[1,1,1,-1]), torch.reshape(sines,[1,1,1,-1])], dim=1)\n",
        "    theta_projs, _ = torch.sort(torch.sum(torch.mul(dgms_big, vecs), dim=1), dim=0)\n",
        "\n",
        "    t1 = torch.reshape(theta_projs, [ccards[-1], -1, 1, thetas.shape[0]])\n",
        "    t2 = torch.reshape(theta_projs, [ccards[-1], 1, -1, thetas.shape[0]])\n",
        "\n",
        "    dists = torch.mean(torch.sum(torch.abs(t1 - t2), dim = 0), dim = 2)\n",
        "    return dists\n",
        "\n",
        "def sliced_wasserstein_distance(dgms, thetas):\n",
        "    # ccards = torch.tensor(np.cumsum([0] + [dgm.shape[0] for dgm in dgms]))\n",
        "    dists = general_sliced_wasserstein_distance(dgms, thetas)\n",
        "    dist = dists[0,1]\n",
        "    return dist"
      ],
      "metadata": {
        "id": "WP3lJqrqcUwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_class_old(diag0_list, diag1_list, parameters, time_subd, label_amount, per_label_amount, m=0.1):\n",
        "\n",
        "  # Needed for sliced wasserstein\n",
        "  thetas = torch.tensor([[1/2, 1/3]]).to(parameters.device)\n",
        "  pd0combi = []\n",
        "  pd1combi = []\n",
        "  for l in range(label_amount):\n",
        "    pd0combi.append(combinations(diag0_list[per_label_amount * l : per_label_amount * (l + 1)], 2))\n",
        "    pd1combi.append(combinations(diag1_list[per_label_amount * l : per_label_amount * (l + 1)], 2))\n",
        "\n",
        "  pd0combi_all = combinations(diag0_list, 2)\n",
        "  pd1combi_all = combinations(diag1_list, 2)\n",
        "\n",
        "  loss = 0\n",
        "  for l in range(label_amount):\n",
        "    loss0_num = 0\n",
        "    loss0_denum = 0\n",
        "    loss1_num = 0\n",
        "    loss1_denum = 0\n",
        "    for diagi, diagj in pd0combi[l]:\n",
        "      dists = [sliced_wasserstein_distance([diagi, diagj], theta) for theta in thetas]\n",
        "      loss0_num += dists[0]\n",
        "\n",
        "    for diagi, diagj in pd1combi[l]:\n",
        "      dists = [sliced_wasserstein_distance([diagi, diagj], theta) for theta in thetas]\n",
        "      loss1_num += dists[0]\n",
        "\n",
        "    for diagi in diag0_list[per_label_amount * l : per_label_amount * (l + 1)]:\n",
        "      for diagj in diag0_list:\n",
        "        if not torch.equal(diagi, diagj):\n",
        "          dists = [sliced_wasserstein_distance([diagi, diagj], theta) for theta in thetas]\n",
        "          loss0_denum += dists[0]\n",
        "\n",
        "    for diagi in diag1_list[per_label_amount * l : per_label_amount * (l + 1)]:\n",
        "      for diagj in diag1_list:\n",
        "        if not torch.equal(diagi, diagj):\n",
        "          dists = [sliced_wasserstein_distance([diagi, diagj], theta) for theta in thetas]\n",
        "          loss1_denum += dists[0]\n",
        "\n",
        "    loss += loss0_num/loss0_denum + loss1_num/loss1_denum\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "pX3HyzlkaA13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_class(diag0_list, diag1_list, parameters, time_subd, label_indices, r_list, m=0.1):\n",
        "\n",
        "  # Needed for sliced wasserstein\n",
        "  thetas = torch.tensor([[1/3], [1/2]]).to(parameters.device)\n",
        "\n",
        "  # Calculate the loss\n",
        "  loss = 0\n",
        "  for label_indx in range(len(r_list)):\n",
        "\n",
        "    # Initiate the numbers\n",
        "    loss0_num = 0\n",
        "    loss0_denum = 0\n",
        "    loss1_num = 0\n",
        "    loss1_denum = 0\n",
        "\n",
        "    # Extract the diagrams from\n",
        "    indices = label_indices[label_indx].tolist()\n",
        "    diagrams0 = [diag0_list[i] for i in indices]\n",
        "    diagrams1 = [diag1_list[i] for i in indices]\n",
        "\n",
        "    dist0 = general_sliced_wasserstein_distance(diagrams0, thetas)\n",
        "    loss0_num += torch.sum(dist0)/2\n",
        "\n",
        "    dist1 = general_sliced_wasserstein_distance(diagrams1, thetas)\n",
        "    loss1_num += torch.sum(dist1)/2\n",
        "\n",
        "    for diagi in diagrams0:\n",
        "      pd0full = [diagi] + diag0_list\n",
        "      dists = general_sliced_wasserstein_distance(pd0full, thetas)\n",
        "      loss0_denum += torch.sum(dists[0, :])\n",
        "\n",
        "    for diagi in diagrams1:\n",
        "      pd1full = [diagi] + diag1_list\n",
        "      dists = general_sliced_wasserstein_distance(pd1full, thetas)\n",
        "      loss1_denum += torch.sum(dists[0, :])\n",
        "\n",
        "    loss += loss0_num/loss0_denum + loss1_num/loss1_denum\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ATNRVe4Bfwxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning pipeline"
      ],
      "metadata": {
        "id": "1QUzY2h425ZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first construct a function that inputs the `coords` and tries to find the best parameters to choose with respect to `loss_class`. Keep in mind that during training, you will actually only get a subset of coords."
      ],
      "metadata": {
        "id": "QS6z0EVV3BYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from tqdm import tqdm\n",
        "\n",
        "def optim_gamma_param(coords, init_param, time_subd, r_list, epochs=100, lr=1, decay_speed=30, m=0.1):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    - Coords: torch tensor\n",
        "    - init_param: torch tensor with required_grad=True\n",
        "  \"\"\"\n",
        "\n",
        "  # Assertions\n",
        "  assert isinstance(coords, torch.Tensor), f\"Expected coords to be a torch.Tensor, got: {type(coords)}.\"\n",
        "  assert isinstance(init_param, torch.Tensor), f\"Expected init_param to be a torch.Tensor, got: {type(init_param)}.\"\n",
        "  assert isinstance(time_subd, torch.Tensor), f\"Expected time_subd to be a torch.Tensor, got: {type(time_subd)}.\"\n",
        "  assert init_param.requires_grad, f\"Expected init_param.requires_grad to be True, got: {init_param.requires_grad}.\"\n",
        "\n",
        "  # Set up optimizer for SGD\n",
        "  opt = torch.optim.SGD([init_param], lr=lr)\n",
        "  scheduler = LambdaLR(opt,[lambda epoch: decay_speed/(decay_speed+epoch)])\n",
        "\n",
        "  # Get unique labels\n",
        "  unique_labels = torch.unique(coords[:, 0])\n",
        "\n",
        "  # Create lookup table\n",
        "  label_indices = {}\n",
        "\n",
        "  # Initialize best loss and parameters\n",
        "  best_loss = 20\n",
        "  best_param = 0\n",
        "\n",
        "  # loop over each unique label\n",
        "  for i, unique_label in enumerate(unique_labels):\n",
        "      indices = (coords[:, 0] == unique_label).nonzero(as_tuple=True)[0]\n",
        "      label_indices[int(i)] = indices\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "\n",
        "    # Create persistence diagram\n",
        "    diag0_list, diag1_list = create_pd_collec(coords, init_param, time_subd, m)\n",
        "\n",
        "    # Calculate the optimal list of parameters\n",
        "    loss = loss_class(diag0_list, diag1_list, init_param, time_subd, label_indices, r_list, m)\n",
        "\n",
        "    # Check if this loss is the best\n",
        "    if loss <= best_loss:\n",
        "      best_loss = loss\n",
        "      best_param = init_param\n",
        "\n",
        "    # Calculate gradient\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # Apply constraints\n",
        "    init_param[0].data.clamp_(min=0)\n",
        "    init_param[0].data.clamp_(max=0.15)\n",
        "    init_param[1].data.clamp_(min=0.00001)\n",
        "    init_param[2].data.clamp_(min=0.00001)\n",
        "    init_param[3].data.clamp_(min=0.00001)\n",
        "    init_param[4].data.clamp_(min=0.00001)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\"Loss\", loss)\n",
        "    print(\"Parameters\", init_param)\n",
        "\n",
        "  return best_param, best_loss"
      ],
      "metadata": {
        "id": "aFKggJpo25KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Run on GPU if possible\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "\n",
        "# # Initialize parameters\n",
        "# coords_tensor = torch.from_numpy(coords)\n",
        "# init_param = torch.tensor([0.1, 0.1, 0.1, 0.1, 0.1], requires_grad=True).to(device)\n",
        "# time_subd = torch.tensor([0., 0.1, 0.2, 0.3]).to(device)\n",
        "# m = torch.tensor(0.1).to(device)\n",
        "\n",
        "# best_param, best_loss = optim_gamma_param(coords_tensor, init_param, time_subd, r_list, epochs=100, lr=1, decay_speed=30, m=0.1)"
      ],
      "metadata": {
        "id": "9mQwr7aHMOSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize a KFold object\n",
        "kf = StratifiedKFold(n_splits=10)\n",
        "\n",
        "# Initialize a list to store scores\n",
        "scores = []\n",
        "\n",
        "# Manipulations to original orbits\n",
        "coords_tensor = torch.from_numpy(coords)\n",
        "pre_labels = coords[:, 0].reshape(-1, 1)\n",
        "pre_features = coords[:, 1:]\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 5\n",
        "decay_speed = 30\n",
        "lr = 1\n",
        "bandwidth = 0.005\n",
        "grid_size = 20\n",
        "resolution = (grid_size, grid_size)\n",
        "\n",
        "\n",
        "# Run on GPU if possible\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for train_index, test_index in kf.split(pre_features, pre_labels):\n",
        "\n",
        "  print(test_index)\n",
        "  # Get training and testing coordinates\n",
        "  train_coords = np.concatenate((pre_labels[train_index], pre_features[train_index]), 1)\n",
        "  test_coords = np.concatenate((pre_labels[test_index], pre_features[test_index]), 1)\n",
        "\n",
        "  # Initialize parameters\n",
        "  train_coords_tensor = torch.from_numpy(train_coords)\n",
        "  best_param = torch.tensor([0.1, 0.1, 0.1, 0.1, 0.1])\n",
        "  time_subd = torch.tensor([0., 0.1, 0.2, 0.3])\n",
        "  m = torch.tensor(0.1)\n",
        "\n",
        "  if best_param.requires_grad:\n",
        "    best_param = best_param.detach()\n",
        "  diag0_list, diag1_list = create_pd_collec(coords_tensor, best_param, time_subd, m)\n",
        "\n",
        "\n",
        "  # Remove requires_grad from empty diagrams\n",
        "  # diag1_list[:] = [diag1.detach() if diag1.requires_grad else diag1 for diag1 in diag1_list]\n",
        "\n",
        "  # Create PI's\n",
        "  pi0_array = PersistenceImage(resolution=resolution, bandwidth=bandwidth).fit_transform(diag0_list)\n",
        "  pi1_array = PersistenceImage(resolution=resolution, bandwidth=bandwidth).fit_transform(diag1_list)\n",
        "\n",
        "  # Concatenate all the persistence diagram per orbit\n",
        "  features = np.concatenate((pi0_array, pi1_array), 1)\n",
        "\n",
        "  labels_to_int = {val: idx for idx, val in enumerate(np.unique(coords[:0]))}\n",
        "  labels = coords[:, 0].astype(int)\n",
        "\n",
        "  # Split into test and train\n",
        "  train_features, test_features = features[train_index], features[test_index]\n",
        "  train_labels, test_labels = labels[train_index], labels[test_index]\n",
        "\n",
        "  # Instantiate model with 100 decision trees\n",
        "  rf = RandomForestClassifier(n_estimators = 100, random_state=42)\n",
        "\n",
        "  # Train the model on training data\n",
        "  rf.fit(train_features, train_labels)\n",
        "\n",
        "  # Evaluate the model on the test data and append the score to scores list\n",
        "  scores.append(rf.score(test_features, test_labels))\n",
        "\n",
        "# Calculate the average accuracy and standard deviation\n",
        "average_accuracy = np.mean(scores)\n",
        "std_accuracy = np.std(scores)\n",
        "\n",
        "# Print the average accuracy and standard deviation\n",
        "print(\"Average Accuracy:\", average_accuracy)\n",
        "print(\"Standard Deviation:\", std_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obi3PogYRZRh",
        "outputId": "25e788c1-4de0-4f88-8d5c-f28fb9e9583b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "[  0   1   2   3   4  50  51  52  53  54 100 101 102 103 104 150 151 152\n",
            " 153 154 200 201 202 203 204]\n",
            "[  5   6   7   8   9  55  56  57  58  59 105 106 107 108 109 155 156 157\n",
            " 158 159 205 206 207 208 209]\n",
            "[ 10  11  12  13  14  60  61  62  63  64 110 111 112 113 114 160 161 162\n",
            " 163 164 210 211 212 213 214]\n",
            "[ 15  16  17  18  19  65  66  67  68  69 115 116 117 118 119 165 166 167\n",
            " 168 169 215 216 217 218 219]\n",
            "[ 20  21  22  23  24  70  71  72  73  74 120 121 122 123 124 170 171 172\n",
            " 173 174 220 221 222 223 224]\n",
            "[ 25  26  27  28  29  75  76  77  78  79 125 126 127 128 129 175 176 177\n",
            " 178 179 225 226 227 228 229]\n",
            "[ 30  31  32  33  34  80  81  82  83  84 130 131 132 133 134 180 181 182\n",
            " 183 184 230 231 232 233 234]\n",
            "[ 35  36  37  38  39  85  86  87  88  89 135 136 137 138 139 185 186 187\n",
            " 188 189 235 236 237 238 239]\n",
            "[ 40  41  42  43  44  90  91  92  93  94 140 141 142 143 144 190 191 192\n",
            " 193 194 240 241 242 243 244]\n",
            "[ 45  46  47  48  49  95  96  97  98  99 145 146 147 148 149 195 196 197\n",
            " 198 199 245 246 247 248 249]\n",
            "Average Accuracy: 0.9279999999999999\n",
            "Standard Deviation: 0.04308131845707602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize a KFold object\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "# Initialize a list to store scores\n",
        "scores = []\n",
        "\n",
        "# Manipulations to original orbits\n",
        "coords_tensor = torch.from_numpy(coords)\n",
        "pre_labels = coords[:, 0].reshape(-1, 1)\n",
        "pre_features = coords[:, 1:]\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 5\n",
        "decay_speed = 30\n",
        "lr = 1\n",
        "bandwidth = 0.005\n",
        "grid_size = 20\n",
        "resolution = (grid_size, grid_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "best_param = torch.tensor([0.1, 0.1, 0.1, 0.1, 0.1])\n",
        "time_subd = torch.tensor([0., 0.1, 0.2, 0.3])\n",
        "m = torch.tensor(0.1)\n",
        "\n",
        "diag0_list, diag1_list = create_pd_collec(coords_tensor, best_param, time_subd, m)\n",
        "\n",
        "\n",
        "# Create PI's\n",
        "pi0_array = PersistenceImage(resolution=resolution, bandwidth=bandwidth).fit_transform(diag0_list)\n",
        "pi1_array = PersistenceImage(resolution=resolution, bandwidth=bandwidth).fit_transform(diag1_list)\n",
        "\n",
        "# Concatenate all the persistence diagram per orbit\n",
        "features = np.concatenate((pi0_array, pi1_array), 1)\n",
        "\n",
        "labels_to_int = {val: idx for idx, val in enumerate(np.unique(coords[:0]))}\n",
        "labels = coords[:, 0].astype(int)\n",
        "print(labels)\n",
        "\n",
        "# Instantiate model with 100 decision trees\n",
        "rf = RandomForestClassifier(n_estimators = 100, random_state=42)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(rf, features, labels, cv=10)\n",
        "\n",
        "# Calculate the average accuracy and standard deviation\n",
        "average_accuracy = np.mean(scores)\n",
        "std_accuracy = np.std(scores)\n",
        "\n",
        "# Print the average accuracy and standard deviation\n",
        "print(\"Average Accuracy:\", average_accuracy)\n",
        "print(\"Standard Deviation:\", std_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdAAU-_xUND3",
        "outputId": "27d72ae5-cd94-4884-8e10-4bafdef75b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
            "Average Accuracy: 0.804\n",
            "Standard Deviation: 0.07031358332498779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test for time & Archive"
      ],
      "metadata": {
        "id": "UY7jCm6G2qA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run on GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "parameters = torch.tensor([0.1, 0.1, 0.1, 0.1, 0.1]).to(device)\n",
        "time_subd = torch.tensor([0., 0.1, 0.2, 0.3]).to(device)\n",
        "m = torch.tensor(0.1).to(device)\n",
        "\n",
        "# Create label_indices\n",
        "# get unique labels\n",
        "unique_labels = torch.unique(coords_tensor[:, 0])\n",
        "\n",
        "# create lookup table\n",
        "label_indices = {}\n",
        "\n",
        "# loop over each unique label\n",
        "for i, unique_label in enumerate(unique_labels):\n",
        "    indices = (coords_tensor[:, 0] == unique_label).nonzero(as_tuple=True)[0]\n",
        "    label_indices[int(i)] = indices\n",
        "\n",
        "\n",
        "import timeit\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "diag0_list, diag1_list = create_pd_collec(coords_tensor, parameters, time_subd, m)\n",
        "print(f\"Time to create diagrams: {timeit.default_timer() - start_time}\")\n",
        "start_time = timeit.default_timer()\n",
        "loss = loss_class(diag0_list, diag1_list, parameters, time_subd, label_indices, r_list, m)\n",
        "print(f\"Class time: {timeit.default_timer() - start_time}\")"
      ],
      "metadata": {
        "id": "B2xc5QG7OM6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dce586e-b48b-4099-977c-6aee0fd2dff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Time to create diagrams: 1.0257181000006312\n",
            "Class time: 4.076312782999594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "d3rnGRZOO3A8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0018656e-8de6-44e0-f6a4-a8a6e57e6f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8915)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foUcfbDYVrG6",
        "outputId": "a55362af-eada-4a8c-91bc-a0dd9d955b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{2.5: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
              " 3.5: tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n",
              " 4.0: tensor([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
              " 4.099999904632568: tensor([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),\n",
              " 4.300000190734863: tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.tensor([[2,3],[5,6],[7,8]])\n",
        "y = torch.tensor([[0,0,1,2,3,5]])"
      ],
      "metadata": {
        "id": "ScTfapKs8Txh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.tensor([0, 2])\n",
        "\n",
        "diag_list =\n",
        "x[z]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efyW7gES8e4S",
        "outputId": "5d604258-95d5-4c76-ffdc-d508735aa54c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "coords_tensor = torch.from_numpy(coords).to(device)\n",
        "unique_labels = torch.unique(coords_tensor[:, 0])\n",
        "\n",
        "# create lookup table\n",
        "label_indices = {}\n",
        "\n",
        "# loop over each unique label\n",
        "for i, unique_label in enumerate(unique_labels):\n",
        "    indices = (coords_tensor[:, 0] == unique_label).nonzero(as_tuple=True)[0]\n",
        "    label_indices[unique_label.item()] = indices\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGdQtgxf8rqj",
        "outputId": "7ae1f656-f414-4806-86b4-9df35a1259d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get diagrams for label 0\n",
        "indices = label_indices[r_list[0]].tolist()\n",
        "diagrams = [diag0_list[i] for i in indices]"
      ],
      "metadata": {
        "id": "kQd9S69AQgSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qULaiUeIQhn3",
        "outputId": "96ed3aa0-58bb-4b9a-8586-96b74974e015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diag0_list[:10] == diagrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C05VvbHbRws7",
        "outputId": "04fd3fac-948a-4df2-d41d-2fd8943670e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([2.4], requires_grad=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUW_eoUWR1rc",
        "outputId": "277e5aff-f845-4a60-b78c-bef310a65a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[2.,3.], [3.,4.]])\n",
        "y = np.array([[9.], [8.]])\n",
        "\n",
        "np.concatenate((y,x), 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpTODXcpI02x",
        "outputId": "59297563-cd21-434f-ff1c-8af3e4a85eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9., 2., 3.],\n",
              "       [8., 3., 4.]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A single diagram with 4 points\n",
        "D1 = torch.tensor([[0.,4.],[1.,2.],[3.,8.],[6.,8.]])\n",
        "D2 = torch.tensor([[0.,5.],[1.,3.],[3.,6.],[6.,7.]])\n",
        "D3 = torch.tensor([[0.,5.],[1.,3.],[3.,6.],[6.,7.]])\n",
        "diags = [D1, D2, D3]\n",
        "\n",
        "pi = PersistenceImage(resolution=[20,20]).fit_transform(diags)\n",
        "\n",
        "# A single diagram with 4 points\n",
        "D1 = torch.tensor([[0.,4.],[1.,2.],[3.,8.],[6.,8.]])\n",
        "D2 = torch.tensor([[0.,5.],[1.,3.],[3.,6.],[6.,7.]])\n",
        "D3 = torch.tensor([[0.,5.],[1.,3.],[3.,6.],[6.,7.]])\n",
        "diags = [D1, D2, D3]\n",
        "\n",
        "pi2 = PersistenceImage(resolution=[20,20]).fit_transform(diags)"
      ],
      "metadata": {
        "id": "7nSCt60afZam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.concatenate((pi, pi2), 1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLZxpEgpATYC",
        "outputId": "92d52e5e-0523-4606-d4a5-d1fdaf551703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 800)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a tensor with requires_grad=True\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# Detach the tensor from the computation graph and set requires_grad to False\n",
        "x = x.detach()"
      ],
      "metadata": {
        "id": "MHsUPyY4ATyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa-JwJSAC_EH",
        "outputId": "b575cda9-fce2-4fb7-dae3-b472e7151e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diag1_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLUULBQXDAJY",
        "outputId": "a7fc2f6d-a945-4859-bbd8-2b0e778d5f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0.1743, 0.1792],\n",
              "         [0.1845, 0.2022],\n",
              "         [0.1917, 0.2080]]),\n",
              " tensor([[0.1645, 0.2121],\n",
              "         [0.2181, 0.2310],\n",
              "         [0.2022, 0.2353]]),\n",
              " tensor([[0.1323, 0.1527]]),\n",
              " tensor([[0.1298, 0.1397],\n",
              "         [0.1490, 0.1667]]),\n",
              " tensor([[0., 0.]], requires_grad=True),\n",
              " tensor([[0., 0.]], requires_grad=True),\n",
              " tensor([[0.1782, 0.1908]]),\n",
              " tensor([[0.1130, 0.1137]]),\n",
              " tensor([[0.0934, 0.0963]]),\n",
              " tensor([[0.1145, 0.1160],\n",
              "         [0.1757, 0.1760]]),\n",
              " tensor([[0.1529, 0.1563],\n",
              "         [0.1515, 0.1564],\n",
              "         [0.1654, 0.1676],\n",
              "         [0.1482, 0.1728]]),\n",
              " tensor([[0.1137, 0.1149]]),\n",
              " tensor([[0.1513, 0.1533],\n",
              "         [0.1512, 0.1565]]),\n",
              " tensor([[0.1687, 0.1766]]),\n",
              " tensor([[0.1634, 0.1700]]),\n",
              " tensor([[0.1244, 0.1275],\n",
              "         [0.1672, 0.1701]]),\n",
              " tensor([[0.1488, 0.1579],\n",
              "         [0.1652, 0.1850]]),\n",
              " tensor([[0.1363, 0.1385]]),\n",
              " tensor([[0., 0.]], requires_grad=True),\n",
              " tensor([[0.1323, 0.1407],\n",
              "         [0.1345, 0.1410]]),\n",
              " tensor([[0.1425, 0.1467],\n",
              "         [0.1639, 0.1651],\n",
              "         [0.1608, 0.1680],\n",
              "         [0.1462, 0.1873]]),\n",
              " tensor([[0., 0.]], requires_grad=True),\n",
              " tensor([[0.1674, 0.1772],\n",
              "         [0.2318, 0.2447]]),\n",
              " tensor([[0.1361, 0.1404],\n",
              "         [0.1686, 0.1716]]),\n",
              " tensor([[0.1446, 0.1553],\n",
              "         [0.1419, 0.1575],\n",
              "         [0.1979, 0.2106]]),\n",
              " tensor([[0.1419, 0.1433],\n",
              "         [0.1375, 0.1498],\n",
              "         [0.1419, 0.1668]]),\n",
              " tensor([[0.1533, 0.1845]]),\n",
              " tensor([[0., 0.]], requires_grad=True),\n",
              " tensor([[0.1561, 0.1652],\n",
              "         [0.1517, 0.1667]]),\n",
              " tensor([[0.1141, 0.1142],\n",
              "         [0.1306, 0.1385],\n",
              "         [0.1300, 0.1519],\n",
              "         [0.1439, 0.1575]]),\n",
              " tensor([[0.1189, 0.1227],\n",
              "         [0.1285, 0.1544]]),\n",
              " tensor([[0.1217, 0.1217],\n",
              "         [0.1458, 0.1671]]),\n",
              " tensor([[0.1346, 0.1505],\n",
              "         [0.1449, 0.1538],\n",
              "         [0.1629, 0.1661]]),\n",
              " tensor([[0.1271, 0.1300],\n",
              "         [0.1623, 0.1703]]),\n",
              " tensor([[0.1599, 0.1757],\n",
              "         [0.1765, 0.1823],\n",
              "         [0.1594, 0.1852]]),\n",
              " tensor([[0.1413, 0.1760]]),\n",
              " tensor([[0.1332, 0.1426],\n",
              "         [0.1439, 0.1462],\n",
              "         [0.1409, 0.1484]]),\n",
              " tensor([[0.1417, 0.1535],\n",
              "         [0.1396, 0.1576]]),\n",
              " tensor([[0.1335, 0.1684],\n",
              "         [0.1686, 0.1800]]),\n",
              " tensor([[0., 0.]], requires_grad=True),\n",
              " tensor([[0.1740, 0.1933]]),\n",
              " tensor([[0.1532, 0.1766],\n",
              "         [0.1515, 0.1971],\n",
              "         [0.1744, 0.1983]]),\n",
              " tensor([[0.1402, 0.1436],\n",
              "         [0.1856, 0.1903],\n",
              "         [0.1684, 0.1918]]),\n",
              " tensor([[0., 0.]], requires_grad=True),\n",
              " tensor([[0.1319, 0.1326],\n",
              "         [0.1696, 0.2036]]),\n",
              " tensor([[0.1450, 0.1534],\n",
              "         [0.1722, 0.2225]]),\n",
              " tensor([[0.2004, 0.2057],\n",
              "         [0.2228, 0.2345]]),\n",
              " tensor([[0.1355, 0.1384],\n",
              "         [0.1408, 0.1620],\n",
              "         [0.1481, 0.1640],\n",
              "         [0.1664, 0.1703]]),\n",
              " tensor([[0.1502, 0.1513],\n",
              "         [0.1582, 0.1669]]),\n",
              " tensor([[0.1260, 0.1375],\n",
              "         [0.1690, 0.2259]])]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1, 3, 5]\n",
        "b = a.copy()\n",
        "a[:] = [x + 2 for x in a]\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsNKHAEJFYub",
        "outputId": "e5197480-e65c-403e-9ec1-df4aec343b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 5, 7]\n",
            "[1, 3, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UZfZj9qdG0-h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}